{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eMRQTYc22AWR",
        "QZHIJjXH2dsM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2: Natural Language Processing\n",
        "Authors: Zechen Wu, Elena Franchini"
      ],
      "metadata": {
        "id": "gurd24wBp9jt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Investigate dataset"
      ],
      "metadata": {
        "id": "irVwI6K7mKcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset selection\n",
        "The dataset we will use is the \"SQuAD2.0: The Stanford Question Answering Dataset\". The website provides the training and validation (i.e. development) set in the form of JSON.\n"
      ],
      "metadata": {
        "id": "-qi67ytW1lZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset analysis\n",
        "Data in the training set consists of strings which represent questions and answers (that come from Wikipedia articles) and can be found as values under the 'data' key. Each 'title' key is associated to a 'paragraphs' key which is an array containing these questions and answers associated to that title (the title acts as a category). Each question is composed by the text representing the question, the id, an array of answers and a flag checking if answering to that question is impossible: if the flag is true, the array of answers is empty. In addition, each answer is associated to the 'answer_start' key whose value represent the starting position of the answer."
      ],
      "metadata": {
        "id": "aubwFnNu1wvt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4X4cGCDrnz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec embedding (or index documents)"
      ],
      "metadata": {
        "id": "NJyRdhlT15i_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluate model"
      ],
      "metadata": {
        "id": "eMRQTYc22AWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model to perform the specific task"
      ],
      "metadata": {
        "id": "OLHEJ60m2FSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test pre-trained models on the task if they already exist"
      ],
      "metadata": {
        "id": "-jabxWia2RLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investigate the effectiveness of Large Language Models (LLMs) together with zero-shot and/or few-shot learning on the task"
      ],
      "metadata": {
        "id": "0PTHcnSC2UnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the different methods and compare their performance across a representative test set"
      ],
      "metadata": {
        "id": "N6qr_Yy12Y5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add voice interactivity"
      ],
      "metadata": {
        "id": "QZHIJjXH2dsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investigate how effective and reliable the voice interactive components are"
      ],
      "metadata": {
        "id": "lh7JzBU32niA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If they are not particularly reliable, how might you change them to make them more robust?"
      ],
      "metadata": {
        "id": "fZP9NMhm2pPU"
      }
    }
  ]
}